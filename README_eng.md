
# Deploy_Yolov4_On_Ultra96_v2
Model deployment,  Yolov4,  Xilinx,  Ultra96_v2.


<div align="center">
ğŸ“– Github
&emsp;&emsp; | &emsp;&emsp;
<a href="https://github.com/chumingqian/Deploy_Yolov4_On_Ultra96_v2/">ğŸ“š Docsify</a>
</div> 
<br>

<div align="center">
ç®€ä½“ä¸­æ–‡
&emsp;&emsp; | &emsp;&emsp;
<a href="https://github.com/chumingqian/Deploy_Yolov4_On_Ultra96_v2/blob/main/README_eng.md">English</a>
</div> 
<br>

This repository has contain the following partï¼š
------------
* Part1: Modify the network  -- yolov4.cfg. 
* Part2: Use Vitis-ai tool to quantify and compile the yolov4 network. 
* Part3: Deploy the yolov4 to the edge device(ultra_96_v2)ä¸Š, write the notebook.ipynb to call the pynq-dpu and inference the network. 
    
 Note: Before we deploy the YOLOV4, some friends may want pruning the YOLOV4 network, there is a reference (https://github.com/chumingqian/Model_Compression_For_YOLOV4), we use this method pruning the network and deployed the pruned yolov4 network successfully. Here is weights both for original network and pruned networkï¼šhttps://pan.baidu.com/s/1lL1tPSOKJc4V4eF_SqVoHw ,code: rvrg, And the yolov4.cfg which suit the dpu are locate at 07-yolov4-tutorial/dk_model/ .  Notice that, we need modify the yolov4.cfg firstly so that network can call the dpu module, then we can prune the modified yolov4.cfg.
  

Part1:  Modify yolov4.cfg network.
------------ 
   
   1.0 Due to the current pynq-DPU 1.2 version doesn't support MISH activation, and the maximum kernel size of the maxpooling  is only support to 8*8 ; In this repository, we made two modifications to enable compliance with  the Zynq Ultrascale + DPU, and also the modified the network can be  quantized and compiled by the  Vitis Ai tool. 
   
               m1  The MISH acitvation are  swapped to leakrelu.      
               m2  The SPP Moudle maxpool sizes have been changed from 5 ,9,13 to 5,5,7.
         


Part2: On Host machine(ubuntu18.04) use the Vitis -ai 1.3.2(Xilinx) tool to quantize and compile the network.
------------

   2.0 Install Docker on Ubuntu18.04, if Docker not installed on your machine (https://docs.docker.com/engine/install/ubuntu/ ). Follow the Post installation steps  for Linux ( https://docs.docker.com/engine/install/linux-postinstall/ ) to ensure that your Linux user is in the group Docker. Or you can  reference the  https://www.xilinx.com/html_docs/vitis_ai/1_3/installation.html to install the vitis aiã€‚
   
   2.1 Clone the Vitis ai repository to obtain the examples, reference code, and scripts.
   
	          git clone --recurse-submodules https://github.com/Xilinx/Vitis-AI  
            
            cd Vitis-AI

   2.2 Run the CPU image from the Docker Hub: (Pelese prepare the above 32G Memory, if you want build the CPU image locally.)
   
	          ./docker pull xilinx/vitis-ai-cpu:latest  
		  
	     Notice that default startup version is the latest, you can add the number if you want start the specified version.
       
		  	         Vitis AI v1.4	./docker_run.sh xilinx/vitis-ai-cpu:1.4.916
				         Vitis AI v1.3	./docker_run.sh xilinx/vitis-ai-cpu:1.3.411
				         Vitis AI v1.3.1
				         Vitis AI v1.3.2
				         Vitis AI v1.2	./docker_run.sh xilinx/vitis-ai-cpu:1.2.82
             
         
   2.3  After we startup the vitis ai, we can see that currently  it support the following deep learning frames:Pytorchã€Tensorflowã€Tensorflow 2 and Caffe .
       In this repository the yolov4 network was trained by the DarkNet, the network file of the modle  is .cfg format, So we both convert the network file's format and weights's format. Here provide the two ways which are convert the Darknet to  Tensorflow and Caffe.
       Then we quantize and compile the converted model, here  is the official  user  guide (https://china.xilinx.com/products/design-tools/vitis/vitis-ai.html), ug1414-vitis-ai.pdf.     
        		

   2.4 Darknet Convert to Tensorflow(conda activate Tensorflow) (For pynq-dpu1.2, generate the dpu_model.elf )
		
		STEP1: convert the model's .cfg file and  model's weights:		
		python ../keras-YOLOv3-model-set/tools/model_converter/convert.py --yolo4_reorder ../dk_model/yolov4-voc-leaky.cfg ../dk_model/leakcy-v4.weights ../keras_model/v4_voc_leaky.h5
		python ../keras-YOLOv3-model-set/tools/model_converter/keras_to_tensorflow.py --input_model ../keras_model/v4_voc_leaky.h5 --output_model=../tf_model/v4_tf_model.pb
	
	   The name of input nodes and output nodes will be different according to the models, use the vai_q_tensorflow  quantizer to check and estimate these nodes: 
     
		$ vai_q_tensorflow inspect --input_frozen_graph=../tf_model/v4_tf_model.pb

		 Otherwise we can visualize the grapth to obtain the name of input nodes and output nodes. 
     TensorBoard and  Netron both support this operation. Here use the Netronï¼š
		$ pip install netron
		$  netron ../tf_model/v4_tf_model.pb

		STEP2: Quantizatinï¼š
		vai_q_tensorflow quantize --input_frozen_graph ../tf_model/v4_tf_model.pb --input_fn yolov4_graph_input_keras_fn.calib_input   --output_dir ../chu_v4_quantized --input_nodes image_input --output_nodes conv2d_93/BiasAdd,conv2d_101/BiasAdd,conv2d_109/BiasAdd --input_shapes ?,416,416,3 --calib_iter 30

		STEP3:COMPLIE 		
		For pynq-dpu1.2 use the following commond, it will generate the .elf  modle.
		dnnc-dpuv2 --save_kernel --parser tensorflow --frozen_pb ../chu_v4_quantized/deploy_model.pb --dcf dpuPynq_ultra96v2.dcf  --cpu_arch arm64 --output_dir ../chu_v4_compiled --net_name tf_model_v4_416

				
   2.5 Darnet  convert to caffe ( conda activate caffe ) (for pynq-dpu1.3, generate the dpu_model.xmodel )	    
		
		STEP1: MODEL CONVERT  TO CAFFE
		python /opt/vitis_ai/conda/envs/vitis-ai-caffe/bin/convert.py ../dk_model/yolov4-voc-leaky.cfg ../dk_model/leakcy-v4.weights  ../dpu1.3.2_caffe_model/v4_leacky.prototxt ../dpu1.3.2_caffe_model/v4_leacky.caffemodel

		STEP2:  MDOEL  Quantization
      *1.Before quantizing the model, we will need to make a minor modifcation to .prototxt file to point to the calibaration images.  Make a new copy of the prototxt file and make the following edits:
            name: "Darkent2Caffe"
            #input: "data"
            #input_dim: 1
            #input_dim: 3
            #input_dim: 416
            #input_dim: 416

            ####Change input data layer to VOC validation images #####
            layer {
              name: "data"
              type: "ImageData"
              top: "data"
              top: "label"
              include {
                phase: TRAIN
              }
              transform_param {
                mirror: false
                yolo_height:416  #change height according to Darknet model
                yolo_width:416   #change width according to Darknet model
              }
              image_data_param {
                source: "voc/calib.txt"  #list of calibration imaages     
                root_folder: "images/" #path to calibartion images

                batch_size: 1
                shuffle: false
              }
            }
            #####No changes to the below layers##### 
    *2. Notice that the calibration images in file.txt, the .txt file needs to be a  two column format to realize the quantization.
    
		*1.åœ¨é‡åŒ–ä¹‹å‰, å¯¹åŸå§‹çš„.prototxtç½‘ç»œæ‹·è´ä¸€ä¸ªå‰¯æœ¬ï¼Œåœ¨å‰¯æœ¬ä¸­åŠ å…¥æ ¡å‡†å›¾ç‰‡çš„è·¯å¾„ï¼Œ ä½¿ç”¨è¯¥å‰¯æœ¬ç½‘ç»œè¿›è¡Œé‡åŒ–ï¼›
		*2.å¹¶ä¸”æ³¨æ„åˆ°æ ¡å‡†å›¾ç‰‡çš„.txt æ–‡æ¡£ä¸­ï¼Œå®ç°é‡åŒ–æ—¶éœ€è¦å«ä¸¤åˆ—çš„åˆ—è¡¨æ–‡ä»¶ï¼Œè¿™ä¸tensorflow çš„æ ¡å‡†æ–‡ä»¶çš„txtæ–‡æ¡£ä¸ä¸€æ ·ã€‚(å¯¹äºé‡åŒ–æ ¡å‡†ï¼Œä¸å«æ ‡ç­¾çš„æ ¡å‡†æ•°æ®å³å¯è¶³å¤Ÿã€‚ä½†å®ç°éœ€è¦å«2åˆ—çš„å›¾åƒåˆ—è¡¨æ–‡ä»¶ã€‚åªéœ€å°†ç¬¬2åˆ—è®¾ä¸ºéšæœºå€¼æˆ– 0 å³å¯)
		*3.æ³¨æ„åˆ°æ ¡å‡†å›¾ç‰‡çš„è·¯å¾„åº”è¯¥æ˜¯docker ç¯å¢ƒä¸‹çš„è·¯å¾„ï¼Œå³è·¯å¾„åº”è¯¥æ˜¯ workspace æ˜¯vitis-ai ä¸ºå·¥ä½œç©ºé—´çš„ï¼Œ æ­¤æ—¶çš„vitis-ai å¯ä»¥ç†è§£æˆä¸»æœºä¸Šçš„home;		
		vai_q_caffe quantize -model ../dpu1.3.2_caffe_model/v4_leacky_quanti.prototxt  -keep_fixed_neuron -calib_iter 3 -weights ../dpu1.3.2_caffe_model/v4_leacky.caffemodel -sigmoided_layers layer133-conv,layer144-conv,layer155-conv -output_dir ../dpu1.3.2_caffe_model/ -method 1 

		STEP3:  MODEL  COMPILE 
		vai_c_caffe --prototxt ../dpu1.3.2_caffe_model/original_model_quanti/deploy.prototxt --caffemodel ../dpu1.3.2_caffe_model/original_model_quanti/deploy.caffemodel --arch ./u96pynq_v2.json --output_dir ../dpu1.3.2_caffe_model/ --net_name dpu1-3-2_v4_voc --options "{'mode':'normal','save_kernel':''}";
    
		 æ³¨æ„åˆ°åœ¨ultra_96_v2ä¸Š,pynq-dpu1.3 ä¸­ï¼Œä½¿ç”¨ç¼–è¯‘ç”Ÿæˆå¥½çš„.xmodel æ–‡ä»¶è¿è¡Œç½‘ç»œæ¨ç†æ—¶ï¼Œ å¦‚æœå‡ºç° footprint  not match çš„ç°è±¡ï¼Œå¯å°†u96pynq_v2.json æ–‡ä»¶æ›¿æ¢æˆ u96pynq.jsonï¼Œå…·ä½“å¯å‚è€ƒï¼šhttps://forums.xilinx.com/t5/AI-and-Vitis-AI/vitis-ai-1-3-with-ultra96/td-p/1189251 ã€‚





Part3: On the edge device (ultra_96_v2),  
åœ¨è¾¹ç¼˜ç«¯(ultra_96_v2), ä½¿ç”¨pynq-dpu1.2 åˆ†åˆ«æµ‹è¯•å‰ªæå‰åyolov4ç½‘ç»œçš„æ¨ç†é€Ÿåº¦ï¼Œä½¿ç”¨pynq-dpu1.3 åˆ†åˆ«æµ‹è¯•å‰ªæå‰åyolov4ç½‘ç»œæ¶ˆè€—çš„èƒ½é‡ã€‚
------------
       3.1  åœ¨SD(32G)å¡ä¸Šçƒ§å†™PYNQ2.6çš„é•œåƒï¼Œ é•œåƒæ–‡ä»¶ï¼ˆhttps://github.com/Xilinx/PYNQ/releases or http://www.pynq.io/board.html) 
       3.2  åœ¨ultra_96_v2 ä¸Šï¼Œè½½å…¥SDå¡ï¼Œ å¯åŠ¨æ¿å¡ã€‚ å¯ä»¥ä½¿ç”¨MobaXtermè¿æ¥ä¸²å£é€šä¿¡ï¼Œ ä»æœ¬åœ°æµè§ˆå™¨ä¸­è¾“å…¥192.168.3.1ï¼› åœ¨æ¿å¡ä¸Šå®‰è£…DPU-PYNQ https://github.com/Xilinx/DPU-PYNQ,  å¦‚æœç½‘é€Ÿè¾ƒæ…¢ï¼Œå¯ä»¥å…ˆä¸‹è½½åˆ°PCç«¯ä¸Šï¼Œ å†ä»PCæœºä¸­æ‹–å…¥åˆ°æ¿å­ä¸­å¯¹åº”çš„è·¯å¾„ä¸‹ã€‚
       3.3  ç¼–å†™ç”¨äºè¿è¡Œç½‘ç»œæ¨ç†çš„notebook.ipynb, ä»¥ä¸‹ä¸ºè°ƒç”¨DPU è¿è¡Œç½‘ç»œæ¨ç†çš„ä¸»ä½“æ­¥éª¤ï¼Œ(å…¶ä¸­æµ‹è¯•åŠŸè€—çš„evaluation.ipynb åœ¨test_energyæ–‡ä»¶ä¸­)ã€‚
                      
			* åŠ è½½æ¨¡å‹(vitis-aiç”Ÿæˆçš„.xmodelæ–‡ä»¶)ï¼š
			  	overlay.load_model(â€œdpu_model.xmodelâ€  or "dup_model.elf")
			* å®šä¹‰dpuå¯¹è±¡
			   	dpu = overlay.runner
			* åˆ›å»ºè¾“å…¥å’Œè¾“å‡ºBuffer
				output_data = [np.empty(shapeOut, dtype=np.float32, order="C")]
				input_data = [np.empty(shapeIn, dtype=np.float32, order="C")]
			* è¿›è¡Œé¢„æµ‹
				job_id = dpu.execute_async(input_data, output_data)
				dpu.wait(job_id)
			* é¢„æµ‹çš„ç»“æœå­˜å‚¨åœ¨output_dataä¸­




Part4: demo.video https://www.bilibili.com/video/BV1AU4y1n7w6/.
------------
å±•ç¤ºäº†å½“ image input size: 416 *416ï¼Œä»ï¼š1.ç½‘ç»œçš„ä½“ç§¯ï¼Œ2.ç½‘ç»œçš„æ¨ç†é€Ÿåº¦ 3.ç½‘ç»œæ¶ˆè€—çš„èƒ½é‡ï¼Œè¿™ä¸‰ä¸ªæ–¹é¢æ¥å¯¹æ¯”å‰ªæå‰åçš„ç½‘ç»œçš„æ€§èƒ½:
 
      1  å¯¹æ¯”å‰ªæå‰åç½‘ç»œæ¨¡å‹çš„ä½“ç§¯å¤§å°.     
      2  åœ¨ultra96_v2, pynq-dpu1.2,çš„ç¯å¢ƒä¸‹è½½å…¥ç”Ÿæˆçš„.elf æ–‡ä»¶ï¼Œè¿è¡Œå¯¹åº”çš„.ipynbæ–‡ä»¶.
         2.1 æµ‹è¯•å‰ªæç½‘ç»œæ¨¡å‹çš„æ¨ç†é€Ÿåº¦ 250 ms.
         2.2 æµ‹è¯•æœªå‰ªæç½‘ç»œæ¨¡å‹çš„æ¨ç†é€Ÿåº¦ 330 ms. 
         
      3  åœ¨ultra96_v2, pynq-dpu1.3,çš„ç¯å¢ƒä¸‹è½½å…¥ç”Ÿæˆçš„.xmodel æ–‡ä»¶ï¼Œè¿è¡Œå¯¹åº”çš„.ipynbæ–‡ä»¶.
         3.1 æµ‹è¯•å‰ªæç½‘ç»œæ¨¡å‹æ¨ç†10 å¼ images æ‰€æ¶ˆè€—çš„åŠŸè€—ï¼Œçº¦ä¸º39J.  éšåæµ‹è¯•æ¨ç†500 imagesï¼Œæ‰€æ¶ˆè€—çš„åŠŸè€—ï¼Œçº¦ä¸º1872J .
         3.2 æµ‹è¯•æœªå‰ªæç½‘ç»œæ¨¡å‹æ¨ç†500 imagesï¼Œæ‰€æ¶ˆè€—çš„åŠŸè€—ï¼Œçº¦ä¸º2347J .
    
                           
#####    å®éªŒç»“æœå¦‚å›¾1æ‰€ç¤ºã€‚

<div align="center">
<img src="./images/fig1.png" width = "700" height = "360" />
</div>	


è‡´è°¢:  æ„Ÿè°¢ XILINX & NICU å…±åŒä¸¾åŠçš„æš‘æœŸå­¦æ ¡ï¼Œè¿™æ˜¯ä¸ªå€¼å¾—çºªå¿µçš„Summer School, æˆ‘ä»¬åº¦è¿‡äº†å—äº¬ç–«æƒ…å’Œä¸Šæµ·â€œçƒŸèŠ±â€å°é£ï¼Œæœ€ç»ˆæŠµè¾¾ XILINX_2021 SUMMER SCHOOLçš„å½¼å²¸. 
======  


<div align="center">
<img src=https://img-blog.csdnimg.cn/20200822014538211.png />
</div>

